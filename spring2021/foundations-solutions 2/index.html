<head>
    <title>Foundations</title>
    <script src="plugins/main.js"></script>
    <script src="grader-all.js"></script>
</head>

<body onload="onLoad('foundations', '<a href=mailto:moojink@stanford.edu>Moo Jin Kim<a>', '03/29/2021', 'https://edstem.org/us/courses/4864/discussion/335099')">

<div id="assignmentHeader"></div>

<p>
    Welcome to your first CS221 assignment!
    The goal of this assignment is to sharpen your math and programming skills
    needed for this class. If you meet the prerequisites, you should find these
    problems relatively innocuous. Some of these problems will occur again
    as subproblems of later homeworks, so make sure you know how to do them.
    If you're unsure about them or need a refresher,
    we recommend going through our prerequisites module or other resources on the Internet,
    or coming to office hours.
    <br>
    <br>
    <b>Before you get started, please read the Homeworks section on the course website thoroughly</b>.
    <br>

</p>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 1: Optimization and probability</div>

<p>
    In this class, we will cast a lot of AI problems as optimization problems, that is, finding the best
    solution in a rigorous mathematical sense.
    At the same time, we must be adroit at coping with uncertainty in the world,
    and for that, we appeal to tools from probability.
</p>

<ol class="problem">

    <li class="writeup" id="1a">
        Let $x_1, \dots, x_n$ be real numbers representing positions on a number line.
        Let $w_1, \dots, w_n$ be positive real numbers representing the importance of each of these positions.
        Consider the quadratic function: $f(\theta) = \frac{1}{2} \sum_{i=1}^n w_i (\theta - x_i)^2$. Note that $\theta$ here is a scalar.
        What value of $\theta$ minimizes $f(\theta)$? Show that the optimum you find is indeed a minimum. What
        problematic issues could arise if some of the $w_i$'s are negative?
        <br><br>Note: You can think about this problem as trying to find the point $\theta$ that's not too far
        away from the $x_i$'s. Over time, hopefully you'll appreciate how nice quadratic functions are to minimize.
        <div class="expected">An expression for the value of $\theta$ that minimizes $f(\theta)$ and how you got it. A short calculation/argument to show that it is a minimum. 1-2 sentences describing a problem that could arise if some of the $w_i$'s are negative.</div>
        <div class="solution">
            A necessary condition for $\theta$ to be a minimizer of a differentiable function $f(\theta)$
            is that its derivative $\frac{d}{d\theta} f(\theta) = 0$.
            Thus, let's take the derivative and set it to zero:
            $\frac{d}{d\theta} f(\theta) = (\sum_{i=1}^n w_i) \theta - \sum_{i=1}^n w_i x_i = 0$.
            We can now solve for $\theta$.
            Doing some basic algebra yields $\theta =
            \frac{\sum_{i=1}^n w_i x_i}{\sum_{i=1}^n w_i}$.
            To double check that $\theta$ is a true minimizer,
            we check that the second derivative is positive. The second derivative is
            $\frac{d^2}{d\theta^2} f(\theta) = \sum_{i=1}^n w_i$, which is positive because of the
            assumption that the $w_i$'s are positive real numbers.
            Note that $\theta$ is just the weighted average of the $x_i$'s,
            which is nice and interpretable.
            If some $w_i$'s are negative, then $f(\theta)$ may not be a convex function (based on the
            second derivative), so a minimizer for this function may not exist.
        </div>
    </li>

    <li class="writeup" id="1b">
        In this class, there will be a lot of sums and maxes.
        Let's see what happens if we switch the order.
        Let $f(\mathbf x) = \sum_{i=1}^d \max_{s_i \in \{1,-1\}} s_i x_i$
        and $g(\mathbf x) = \max_{s \in \{1,-1\}} \sum_{i=1}^d s x_i$,
        where $\mathbf x = (x_1, \dots, x_d) \in \mathbb{R}^d$ is a real vector.
        Which of $f(\mathbf x) \le g(\mathbf x)$, $f(\mathbf x) = g(\mathbf x)$, or $f(\mathbf x) \ge g(\mathbf x)$ is true for
        all $\mathbf x$?
        Prove it.
        <br><br>Hint: You may find it helpful to refactor the expressions so that they
        are maximizing the same quantity over different sized sets.
        <div class="expected">A short (3-5) line/sentence proof. You should use mathematical notation in your proof, but can also make your argument in words.</div>
        <div class="solution">
            Intuitively, for $f(\mathbf x)$, we can select a different $s$ for each $i$,
            whereas for $g(\mathbf x)$, we have to select a single $s$ for all $i$.
            More formally, to make $f(\mathbf x)$ and $g(\mathbf x)$ look more similar for comparison,
            let's rewrite them:
            $f(\mathbf x) = \max_{s_1, \dots, s_n} \sum_{i=1}^d s_i x_i$.
            and
            $g(\mathbf x) = \max_{s_1 = \cdots = s_n} \sum_{i=1}^d s_i x_i$
            Now it's obvious that both functions are maximizing over the same quantity,
            except that $f(\mathbf x)$ is maximizing over a larger set.
            Therefore, we have that $f(\mathbf x) \ge g(\mathbf x)$ for all $\mathbf x$. 
            Alternatively, we can reach the same answer by realizing that in $f(\mathbf x)$,
            we have $\sum_{i=1}^d \lvert x_i \rvert$, while in $g(\mathbf x)$,
            we are taking the max of $\sum_{i=1}^d -x_i$ and $\sum_{i=1}^d x_i$.
        </div>
    </li>

    <li class="writeup" id="1c">
        Suppose you repeatedly roll a fair six-sided die until you roll a $1$ (and then you stop).
        Every time you roll a $2$, you lose $a$ points, and every time you roll a 6, you win $b$ points. You do not win
        or lose any points if you roll a 3, 4, or a 5.
        What is the expected number of points (as a function of $a$ and $b$) you will have when you stop?
        <br><br>Hint: You will find it helpful to define a recurrence. If you define $V$ as the expected number of points you get from playing the game, what happens if you roll a 2? You lose $a$ points and then get to play again. What about the other cases? Can you write this as a recurrence?

        <div class="expected">A recurrence to represent the problem and the resulting expression from solving the recurrence (no more than 1-2 lines).</div>
        <div class="solution">
            Let $V$ be the expected number of points you will earn (the expected value of the game).<br>
            Consider the different possible cases. If you roll a 2, you lose $a$ points, and then get to re-roll/play the same game again, with an expected value of $V$.
            If you roll a 6, the same happens but you gain $b$ points instead. If you roll a 3, 4, or 5, you immediately play the same game again with expected value $V$. If you roll a 1, the game ends immediately and you gain no points and get no chance to play again.<br>
            We can define the recurrence by considering the probability of each outcome and the value of each outcome:
            $V = \frac{1}{6}(V-a) + \frac{1}{6}(V+b) + \frac{3}{6} V + \frac{1}{6}(0) = \frac{1}{6}(-a) + \frac{1}{6}(b) + \frac{5}{6} V$.
            Solving for $V$ yields $b - a$.
            These types of calculations will show up in Markov decision processes, where we
            consider iterated games of chance.
        </div>
    </li>

    <li class="writeup" id="1d">
        Suppose the probability of a coin turning up heads is $0 \lt p \lt 1$,
        and we flip it 7 times and get $\{ \text{H}, \text{H}, \text{T}, \text{H}, \text{T} , \text{T}, \text{H}
        \}$.
        We know the probability (likelihood) of obtaining this
        sequence is $L(p) = p p (1-p) p (1-p) (1-p) p = p^4(1-p)^3$.
        What value of $p$ maximizes $L(p)$?
        Prove/Show that this value of $p$ maximizes $L(p)$.
        What is an intuitive interpretation of this value of $p$?
        <br><br>Hint: Consider taking the derivative of $\log L(p)$. You can also directly take the derivative of $L(p)$,
        but it is cleaner and more natural to differentiate $\log L(p)$. You can verify for yourself
        that the value of $p$ which maximizes $\log L(p)$ must also maximize $L(p)$ (you are not required to prove this
        in your solution).
        <div class="expected">The value of $p$ that maximizes $L(p)$ and the work/calculation used to solve for it. Note that you must prove/show that it is a maximum. A 1-sentence intuitive interpretation of the value of $p$.</div>
        <div class="solution">
            We have $\log L(p) = 4 \log p + 3 \log (1-p)$.
            Taking the derivative yields $\nabla \log L(p) = \frac{4}{p} - \frac{3}{1-p}$.
            Setting this to zero and solving yields $p = \frac{4}{7}$.
            Checking that the second derivative is negative for $0 &lt; p &lt; 1$:
            $\nabla \log L(p) = -4/p^2 - 3/(1-p)^2 &lt; 0$.
            The optimal $p$ has a very intuitive interpretation:
            it's just the fraction of heads.
            This is a classic derivation of the maximum likelihood estimator in statistics.
        </div>
    </li>

    <li class="writeup" id="1e">
        Now for a little bit of practice manipulating conditional probabilities. Suppose that $A$ and $B$ are two events such that $P(A|B) = P(B|A)$. We also know that and $P(A \cup B) = 1$ and $P(A \cap B) > 0$. Prove that $P(A) > \frac{1}{2}$.

        <br><br>Hint: Note that $A$ and $B$ are not necessarily mutually exclusive. Consider how we can relate $P(A \cup B)$ and $P(A \cap B)$.

        <div class="expected">A short (~5 line) proof/derivation.</div>
        <div class="solution">
            We start with Bayes' rule, which tells us that $P(A|B) = \frac{P(B|A) P(A)}{P(B)}$.<br>
            We can divide both sides by $P(A|B)$ to yield $1 = \frac{P(B|A) P(A)}{P(A|B) P(B)}$. Using the fact that $P(A|B) = P(B|A)$, we cancel and rearrange to find $P(A) = P(B)$.<br>

            Next, we consider that $P(A \cup B) = P(A) + P(B) - P(A \cap B)$. Given that $P(A \cup B) = 1$, we can rearrange this as $P(A) + P(B) = 1 + P(A \cap B)$.<br>
            Now, using the fact that $P(A \cap B) > 0$, it holds that $P(A) + P(B) > 1$. Using our earlier finding that $P(A) = P(B)$ allows us to write $P(A) + P(A) > 1$, and therefore $P(A) > \frac{1}{2}$, as desired.
        </div>
    </li>

    <li class="writeup" id="1f">
        Let's practice taking gradients,
        which is a key operation for being able to optimize continuous functions.
        For $\mathbf w \in \mathbb R^d$ (represented as a column vector) and constants $\mathbf a_i, \mathbf b_j \in
        \mathbb R^d$ (also represented as column vectors) and $\lambda \in \mathbb R$, define
        the scalar-valued function
        $$f(\mathbf w) = \Bigg( \sum_{i=1}^n \sum_{j=1}^n (\mathbf a_i^\top \mathbf w - \mathbf b_j^\top \mathbf w)^2 \Bigg) + \lambda
        \|\mathbf w\|_2^2,$$
        where the vector is $\mathbf w = (w_1, \dots, w_d)^\top$ and $\|\mathbf w\|_2 = \sqrt{\sum_{k=1}^d w_k^2} = \sqrt{{\mathbf w}^T {\mathbf w}}$ is
        known as the $L_2$ norm.
        Compute the gradient $\nabla f(\mathbf w)$.
        <br><br>Recall: the gradient is a $d$-dimensional vector of the partial derivatives with respect to each $w_i$:
        $$\nabla f(\mathbf w) = \left(\frac{\partial f(\mathbf w)}{\partial w_1}, \dots \frac{\partial f(\mathbf
        w)}{\partial w_d}\right)^\top.$$
        If you're not comfortable with vector calculus, first warm up by working out this problem using scalars in
        place of vectors and derivatives in place of gradients.
        Not everything for scalars goes through for vectors, but the two should at least be consistent with each other
        (when $d=1$).
        Do not write out summations over dimensions, because that gets tedious.

        <div class="expected">An expression for the gradient and the work used to derive it. (~5 lines). No need to expand out terms unnecessarily; try to write the final answer compactly.</div>
        <div class="solution">
            $\nabla f(\mathbf w) = \nabla (\sum_{i=1}^n \sum_{j=1}^n (\mathbf a_i^\top \mathbf w - \mathbf b_j^\top
            \mathbf w)^2) + \nabla(\lambda \|\mathbf w\|_2^2)$.
            <br/>We know $\nabla \|\mathbf w\|_2^2 = \nabla (\mathbf w^\top \mathbf w) = 2 \mathbf w$.
            <br/>Hence as $\lambda$ is a constant,
            $\nabla f(\mathbf w) = \nabla (\sum_{i=1}^n \sum_{j=1}^n (\mathbf a_i^\top \mathbf w - \mathbf b_j^\top
            \mathbf w)^2) + 2 \lambda \mathbf w$.
            <br/>$\mathbf a_i, \mathbf b_j \in \mathbb R^d$ are constants.
            $\nabla f(\mathbf w) = \sum_{i=1}^n \sum_{j=1}^n \nabla (\mathbf a_i^\top \mathbf w - \mathbf b_j^\top
            \mathbf w)^2 + 2 \lambda \mathbf w$.
            <br/>Applying the chain rule,
            $\nabla f(\mathbf w) = 2 \sum_{i=1}^n \sum_{j=1}^n (\mathbf a_i^\top \mathbf w - \mathbf b_j^\top \mathbf
            w)(\mathbf a_i - \mathbf b_j) + 2 \lambda \mathbf w$.
        </div>
    </li>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 2: Complexity</div>

<p>
    When designing algorithms, it's useful to be able to do quick back-of-the-envelope
    calculations to see how much time or space an algorithm needs.
    Hopefully, you'll start to get more intuition for this by being exposed
    to different types of problems.
</p>

<ol class="problem">

    <li class="writeup" id="2a">
        Suppose we have an $n \times n$ grid, where we'd like to place 6 arbitrary axis-aligned rectangles (i.e., the sides of the rectangle are parallel to the axes).
        There are no constraints on the location or size of the rectangles. For example, it is possible for all four corners of a single rectangle to be the same point (resulting in a rectangle of size 0) or for all 6 rectangles to be on top of each other.
        How many possible ways are there to place 6 rectangles on the grid?
        In general, we only care about asymptotic complexity,
        so give your answer in the form of $O(n^c)$ or $O(c^n)$ for some integer $c$.
        <br><br>Note: It is unnecessary to consider whether order matters in this problem, since we are asking for asymptotic complexity. You are free to assume either in your solution, as it doesnâ€™t change the final answer.

        <div class="expected">A big-O bound for the number of possible ways to place 6 rectangles and some simple explanation/reasoning for the answer (~2 sentences).</div>
        <div class="solution">
            There are $O(n)$ coordinates,
            $O(n^2)$ points (each defined by two coordinates),
            and $O(n^4)$ possible rectangles in the grid (each defined by two corner points).
            Since we need to select $6$ rectangles, the total number of ways is
            $O((n^4)^6) = O(n^{24})$.
        </div>
    </li>

    <li class="writeup" id="2b">
        Suppose we have an $n\times n$ grid.
        We start in the upper-left corner (position $(1,1)$), and we would like to reach the lower-right corner
        (position $(n,n)$) by taking single steps down or to the right.
        Suppose we are provided with a function $c(i, j)$ that outputs the cost of touching position $(i, j)$, and assume it takes constant time to
        compute for each position.
        Note that $c(i, j)$ can be negative.
        Give an algorithm for computing the cost of the minimum-cost path from $(1,1)$ to $(n,n)$ in the most efficient way (with smallest big-O time complexity).
        What is the runtime (just give the big-O)?

        <div class="expected">A description of the algorithm for computing the cost of the minimum-cost path as efficiently as possible (~5 sentences). The big-O runtime and a short explanation of how it arises from the algorithm.</div>
        <div class="solution">
            We can compute the cost of the minimum-cost path by defining the following recurrence: $f(i,j) = c(i,j) +
            \min(f(i-1,j), f(i,j-1))$ for $i,j = 1, \dots, n$.
            The initial state is $f(1,1) = c(1,1)$, and our solution will be found as $f(n,n)$.
            To handle the top and left edges, we can simply define $f(i,j) = \infty$ whenever $i=0$ or $j=0$.
            For each $i,j = 1, \dots, n$, we need to compute $f(i,j)$.
            If we loop through $i$ and $j$ in increasing order, we will have already computed $f(i-1,j)$ and $f(i,j-1)$,
            so we can compute $f(i,j)$ in $O(1)$ time.
            Thus, the total running time is $O(n^2)$.
            Note that this is a basic case where we are memoizing the result of $f(j)$;
            if we didn't do that, then it would take time exponential in $n$, which is too slow.
        </div>
    </li>

</ol>

<!------------------------------------------------------------>
<div class="problemTitle">Problem 3: Programming</div>

<p>
    In this problem, you will implement a bunch of short functions. The main
    purpose of this exercise is to familiarize yourself with Python,
    but as a bonus, the functions that you will implement will come in handy in
    subsequent homeworks.
    <br>
    <br>
    <b>Do not import any outside libraries (e.g. numpy).</b> Only standard Python
    libraries and/or the libraries imported in the starter code are allowed.
</p>

<p>
    If you're new to Python, the following provide pointers to various
    tutorials and examples for the language:
</p>
<ul>
    <li><a href="http://wiki.python.org/moin/BeginnersGuide/Programmers">Python for Programmers</a></li>
    <li><a href="http://wiki.python.org/moin/SimplePrograms">Example programs of increasing complexity</a></li>
</ul>
<div class="expected"> Python code implementing the functions provided in submission.py. Try to make your code as clean and simple as possible and be sure to write your answers between the begin answer and end answer comments.</div>
<ol class="problem">

    <li class="code" id="3a">
        Implement <code>find_alphabetically_last_word</code> in <code>submission.py</code>.
    </li>
    <li class="code" id="3b">
        Implement <code>euclidean_distance</code> in <code>submission.py</code>.
    </li>
    <li class="code" id="3c">
        Implement <code>mutate_sentences</code> in <code>submission.py</code>.
    </li>
    <li class="code" id="3d">
        Implement <code>sparse_vector_dot_product</code> in <code>submission.py</code>.
    </li>
    <li class="code" id="3e">
        Implement <code>increment_sparse_vector</code> in <code>submission.py</code>.
    </li>
    <li class="code" id="3f">
        Implement <code>find_singleton_words</code> in <code>submission.py</code>.
    </li>

</ol>

<div class="problemTitle">Problem 4: Background Survey</div>

<p>
    As a big class and the first AI/ML course taken by many students, CS221 has a wide range of people coming from different backgrounds.
    On top of that, the transition to remote learning has created new challenges for all of us. We know that these challenges affect students unevenly and to different degrees depending on their individual circumstances.
    As a result, we've put together a quick survey to get a little more information about the background of everyone in the class and figure out what we can do to best support each one of you this quarter.
    <br>
    <br>
    Please fill out the Google form <u><a href="https://forms.gle/t13QSyGe4wcFWUk48" target="_blank"> here</a></u>.
    <br>
    <br>
    This survey is optional and will not be graded, so your answers will not affect your grade whatsoever.
    However, it will be a huge help to us in making CS221 a great experience for everyone!

    <div class="expected">Just fill out the survey!</div>
</p>


<div class="problemTitle">Submission</div>
<p>
    Submission is done on Gradescope.

    <br>
    <br>
    <b>Written:</b> When submitting the written parts, make sure to select <b>all</b> the pages
    that contain part of your answer for that problem, or else you will not get credit.
    To double check after submission, you can click on each problem link on the right side, and it should show
    the pages that are selected for that problem.
    <br>
    <br>
    <b>Programming:</b> After you submit, the autograder will take a few minutes to run. Check back after
    it runs to make sure that your submission succeeded. If your autograder crashes, you will receive a 0 on the
    programming part of the assignment.
    <br>
    <br>
    More details can be found in the Submission section on the course website.
</p>

</body>
